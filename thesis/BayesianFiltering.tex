\section{Bayesian Filtering}
\subsection{Overview}

This section will introduce and describe the Bayesian for solving time-dependent inverse problems, laying out the theoretical foundations of the taken approach. In Section 2.3 we reformulate the definition of inverse problems for the finite-dimensional time-dependent case. We then present the pendulum problem, which will be studied along the whole thesis to illustrate important ideas. In Section 2.4 we present first present the classical approach for solving inverse problems and the challenges it encounters with noisy data. We next present the Bayesian approach, and show how it incorporates prior information about the structure of the problem to address uncertainty. We then adapt the definition of the pendulum problem to the Bayesian framework. Finally, Section 2.5 presents important results, including a characterization of the class of well-posed inverse problems. We conclude the section by proving that the pendulum problem is well-posed.

\subsection{Set-Up}

We study parametrized models for which the value of the \textit{parameter} $\theta \in \Theta$ is unknown or uncertain. To model the behaviour of the system, we introduce \textit{forward response operators} $\mathcal{G} : \Theta \rightarrow Y$ mapping values of the \textit{parameter space} to the \textit{data space}, assuming both spaces to be finite-dimensional vector spaces.

We consider a real system described by $\mathcal{G}$ and the true parameter $\theta_{true} \in \Theta$, and assume that \textit{observations} $y \in Y$ of the system are available from measurements. To model the noise often present in such measurements, we treat $y$ as a realization of $\mathcal{G}(\theta_{true}) + \eta$ where $\eta$ is a mean-zero random variable, usually chosen to be Gaussian with covariance matrix $\Gamma$. This leaves us with the equation

\begin{equation}
  y = \mathcal{G}(\theta_{true}) + \eta,
\end{equation}

and the question \textit{To which extent can we find the \textit{inverse} of the data $y$ under the forward response operator $\mathcal{G}$?} Answering this question is known as the \textit{inverse problem}.

In this thesis, we assume a specific structure on the forward response operator and on the data. The systems are assumed to be time-dependent and described as the solution of a deterministic initial value problem of the form

\begin{equation}
  \begin{aligned}
    \frac{\text{d}x}{\text{d}t} &= f(x; \theta)\\
    x(0) &= x_0,
  \end{aligned}
\end{equation}  

where $\theta \in \Theta$ is the parameter of the model, and the solution $x(t; \theta) \in X$ is assumed to exist for every time $t \geq 0$. We further assume the measurements of the system to be sequentially taken at times  $0 \leq t_1 < \ldots < t_N$. We use an \textit{observational operator} $\mathcal{O} : X \rightarrow Y$ to model the measurement procedure, mapping states of the system to observations. We can then define a sequence of forward response operator $\mathcal{G}_i : \Theta \rightarrow Y$ all given by $\mathcal{G}_i = \mathcal{O} \circ x(t_i; \cdot)$. We keep modeling the noise present by treating each observation $y_i$ as the realization of adding Gaussian noise to the forward response operator, now considering the sequence of noise variables $\eta_1, \ldots, \eta_N$ to be i.i.d. mean-zero Gaussian variables. Instead of trying to inverse one equation, we now consider the following set of equations

\begin{equation}
  y_i = \mathcal{G}_i(\theta_{true}) + \eta_i.
\end{equation}

This allows us to reformulate the question from above as \textit{How can we use new observations to update our knowledge about $\theta_{true}$?} Answering this new question is called the \textit{filtering problem}, and solving inserve and filtering problems will be the focus of this thesis.

\subsection{Pendulum problem}

We now introduce a filtering problem that will guide the rest of the thesis. Using a pendulum, we would like to estimate the value of the Earth's gravitational acceleration. To do this, we first had to model the behaviour of the pendulum using a model parametrized by the gravitational acceleration $g$. We chose to use the \textit{simple pendulum model}, a simplified model that ignores the mass of the hanging mass and of the string, ignores the forces of friction present on the hanging mass and assumes the movement on the pendulum is only happening on one plane. This model is illustrated in Figure 1. This simplification allows to model the state of the pendulum with a single value $x(t)$ representing the angle of the pendulum to the resting point, described by the following differential equation

\begin{equation}
  \frac{\text{d}^2x}{\text{d}t^2} = -\frac{g}{l}\sin(x).
\end{equation}

In this model, $g$ is the Earth's gravitational acceleration and $l$ is the length of the string holding the hanging mass.

We proceeded to run an experiment, in which the pendulum was let go from an initial angle $x(0) = 5\frac{\pi}{180}$ and no initial velocity. We then measured the first $N = 11$ times at which the pendulum was aligned with the vertical axis, indicating a null angle. The variance of the Gaussian error was chosen to be of about one percent of a $1^\circ$ angle giving $\sigma_i^2 = 0.001$.

\subsection{Bayesian filtering}

The previous paragraphs focused on giving a definition of inverse and filtering problems. Before starting to discussing frameworks for expressing such problems, we define the useful concept of \textit{well-posedness} first given by Hadamard for describing properties of models of physical phenomena.

\begin{definition}[Well-posedness] A problem is said to be well-posed if it satisfies the following conditions:
  \begin{enumerate}
  \item{a solution exists,}
  \item{the solution is unique,}
  \item{the solution changes continuously with the initial condition.}
  \end{enumerate}

  A problem failing to satisfy these conditions is said to be ill-posed. In the context of inverse problems, the 3rd property should be understand as continuity of the solution of the problem with respect to the data.  
\end{definition}

A possible way to solve inverse problems is to try to find a value $\hat \theta \in \Theta$ that solves the inverse problem \textit{as well as possible}. This is done by replacing the inverse problem by the optimization problem

\begin{equation*}
  \hat\theta = \underset{\theta \in \Theta}{\text{argmin}} \norm{ y - \mathcal{G}(\theta) }_Y.
\end{equation*}

However, finding a global minimum in the presence of noise is often a difficult task since it might not exist, or the minimized function might admit multiple local minima. Solving the inverse problem by minimization is thus an ill-posed problem. While some of these problems can be addresses by \textit{regularization}, two issues remain unresolved. First, regularization and the choice of the minimized norm are ad hoc decisions that are not part of the modeling process but rather tuning parameters of the optimization problem. Then, assuming that the optimization algorithm does provide an estimate $\hat\theta$, this point estimate does not contain any information about the \textit{uncertainty} around this estimation, making it hard to take decisions based on this result.

We choose instead to study Bayesian methods for solving inverse problems.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "Thesis"
%%% End:
