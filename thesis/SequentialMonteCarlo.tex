\section{Proofs}


\begin{theorem}
  \label{duley}
  Let $\mu, \nu$ be probability measures on $S \times T$, where $(S, \mathcal{A})$ and $(T, \mathcal{B})$ are measurable spaces. Let $(x, y) \in S \times T$. Assume that $\mu \ll \nu$ and that $\mu$ has Radon-Nikodym derivative $\phi$ with respect to $\mu$. Assume further that the conditional distributions of $x|y$ under $\nu$, denoted by $\nu^y(\text{d}x)$, exists. Then the conditional distribution of $x|y$ under $\mu$, denoted $\mu^y(\text{d}x)$, exists and $\mu^y(\text{d}x) \ll \nu^y(\text{d}x)$. The Radon-Nikodym derivative is given by

  \begin{equation}
    \frac{\text{d}\nu^y}{\text{d}\mu^y}(x) =  \begin{cases}
      \frac1{c(y)}\phi(x,y) & \text{if $c(y) > 0$, and}\\
      1 & \text{else,}
    \end{cases}  
  \end{equation}

  where $c(y) = \int_{S}\phi(x,y)\text{d}\mu^y(x)$ for all $y \in T$.
\end{theorem}

\begin{proof}
  See \cite{dudley_2002}.
\end{proof}

\begin{theorem}[Generalized Bayes' Rule]
  Assume that $\mathcal{G} : \Theta \rightarrow Y$ is continuous, that $\eta$ has a density $\rho$ with support equal to $Y$ and that $\mu_0(\Theta) = 1$. Then $\theta | y$ is distributed according to the measure $\mu^y$, with $\mu^y \ll \mu_0$ and Radon-Nikodym derivative with respect to $\mu_0$ given by

  \begin{equation}
    \frac{\text{d}\mu^y}{\text{d}\mu_0}(\theta) = \frac1{Z_y}\rho(y - \mathcal{G}(\theta)),
  \end{equation}

  where $Z_y$ is a constant that only dependends on $y$ and not on $\theta$, called the \textit{model evidence}.
\end{theorem}

\begin{proof}
  Let $\mathbb{Q}_0(\text{d}y) = \rho(y)\text{d}y$ and $\mathbb{Q}(dy|\theta) = \rho(y - \mathcal{G}(\theta))$. Since both measures have a Radon-Nikodym derivative with respect to the Lebesgue measure, we have

  \begin{equation*}
    \frac{\text{d}\mathbb{Q}}{\text{d}\mathbb{Q}_0}(y|\theta) = \frac{\text{d}\mathbb{Q}}{\text{d}\lambda}(y|\theta)\left(\frac{\text{d}\mathbb{Q}_0}{\text{d}\lambda}(y)\right)^{-1} = \frac{\rho(y - \mathcal{G}(\theta)}{\rho(y)} =: C(y)\rho(u - \mathcal{G}(\theta)).
  \end{equation*}

  Further define two measures $\nu_0, \nu$ on $Y \times \Theta$ by
  \begin{equation*}
    \begin{aligned}
      \nu_0(\text{d}y, \text{d}\theta) &= \mathbb{Q}_0(\text{d}y) \otimes \mu_0(\text{d}\theta)\\
      \nu(\text{d}y, \text{d}\theta) &= \mathbb{Q}(\text{d}y|\theta) \mu_0(\text{d}\theta).
    \end{aligned}
  \end{equation*}

  Since $\mathcal{G}$ is continuous and $\mu(\Theta) = 1$, it is also $\mu_0$-measurable. Thus, $\nu$ is well-defined and continuous with respect to $\nu_0$ with Radon-Nikodym derivative

  \begin{equation*}
    \frac{\text{d}\nu}{\text{d}\nu_0}(y, \theta) = C(y)\rho(y - \mathcal{G}(\theta)).
  \end{equation*}

  Since $\nu_0$ is a product measure over $Y \times \Theta$, the random variables $y$ and $\theta$ are independent giving $\theta|y = \theta$. This implies that the conditional distribution of $\theta|y$ under $\nu_0$ is then $\nu_0^y = \mu_0$. We further note that since $\rho > 0$ we have $C(y) > 0$ and $\rho(y - \mathcal{G}(\theta)) > 0$ giving

  \begin{equation*}
    c(y) := \int_\Theta C(y)\rho(y - \mathcal{G}(\theta))\mu_0(\text{d}\theta) > 0 
  \end{equation*}


  Thus, by Theorem \ref{duley}, the conditional distribution of $\theta|y$ under $\nu$, denoted $\mu^y$, exists and its Radon-Nikodym derivative with respect to $\nu_0^y = \mu_0$ is

  \begin{equation*}
    \frac{\text{d}\mu^y}{\text{d}\mu_0}(\theta) = \frac1{c(y)}C(y)\rho(y - \mathcal{G}(\theta)) =  \frac1{Z_y}\rho(y - \mathcal{G}(\theta)).
  \end{equation*}
  
  where $Z_y = \int_\Theta\rho(y - \mathcal{G}(\theta))\mu_0(\text{d}\theta)$.
\end{proof}

\begin{lemma}
  Let $P(E)$ denote the set of all probability measures on $E$. For every $\mu$ and $\nu$ random variables with values in $P(E)$, we define

  \begin{equation*}
    d(\mu, \nu) := \underset{f}{\text{sup}} \sqrt{\mathbb{E}\left[(\mu f - \nu f)^2\right]},
  \end{equation*}

  where the supremum is taken over all $f : E \rightarrow \mathbb{R}$ with $|f|_\infty \le 1$, and $\mu f$ denotes the integral of $f$ under $\mu$. Then $d$ is a metric over the space of random measures over $E$.
\end{lemma}

\begin{proof} Trivial enough not skip the proof?
\end{proof}

\begin{definition}
  Let $M \in \mathbb{N}$, for every $\mu \in P(E)$ we define $S^M\mu$ by

  \begin{equation*}
    S^M\mu = \frac1{M}\sum_{i=1}^M\delta_{u_i},
  \end{equation*}

  where $u^{(1)}, \ldots, u^{(M)}$ are i.i.d. random variables distributed according to $\mu$. From the randomness of the samples $u^{(1)}, \ldots, u^{(M)}$, it follows that $S^M\mu$ is a random variable with values in $P(E)$. The operator $\mu \mapsto S^M\mu$ is called the \textit{sampling operator}.
\end{definition}


\begin{lemma}\label{sampling_bound}
  The sampling operator satisfies

  \begin{equation*}
    \underset{\mu \in P}{\text{sup}}\ d(S^M\mu, \mu) \le \frac1{\sqrt{M}}
  \end{equation*}
\end{lemma}

\begin{proof}
  Let $\mu$ be an element of $P(E)$ and $u^{(1)}, \ldots, u^{(M)}$ be i.i.d. random variables distributed according to $\mu$. For every $f$ with $|f|_\infty \le 1$ we have 
  
  \begin{equation*}
    (S^M\mu)f - \mu f = \frac1{M}\sum_{i=1}^Mf(u^{(i)}) - \mu f = \frac1{M}\sum_{i=1}^Mf_i,
  \end{equation*}

  where $f_i = f(u^{(i)}) - \mu f$. This gives

  \begin{equation*}
    \begin{aligned}
      ((S^M\mu - \mu)(f))^2
      &= \left(\frac1{M}\sum_{i=1}^Mf(u^{(i)}) - \mu f\right)^2\\
      &= \frac1{M^2}\sum_{i,j=1}^M(f(u^{(i)}) - \mu f)(f(u^{(j)}) - \mu f)\\
      &= \frac1{M^2}\sum_{i,j=1}^Mf_if_j.
    \end{aligned}
  \end{equation*}

  Since we are interested in the expected value of the initial term, we now consider $\mathbb{E}[f_if_j]$. For $i \neq j$, $f_i$ and $f_j$ are independent random variables, thus $\mathbb{E}[f_if_j] = \mathbb{E}[f_i]\mathbb{E}[f_j]$, and since $u^{(i)} \sim \mu$, we have $\mathbb{E}[f_i] = \mathbb{E}[f(u^{(i)})] - \mu f = 0$, giving $\mathbb{E}[f_if_j] = 0$ for $i \neq j$. Furthermore, since $|f|_\infty \le 1$ we have

  \begin{equation*}
    \mathbb{E}[f_i^2] = \text{Var}[f(u^{(i)})] = \mathbb{E}[f(u^{(i)})^2] - \mathbb{E}[f(u^{(i)})]^2 \leq 1.
  \end{equation*}

  By linearity of the expected value, we then have for every $f$ with $|f|_\infty \le 1$

  \begin{equation*}
    \mathbb{E}[( (S^M\mu)f - \mu f)^2] = \frac1{M^2}\sum_{i=0}^N\mathbb{E}[f_i^2] \le \frac1{M}.
  \end{equation*}

  Taking the square root on both sides of the equation and the supremum over all such $f$ yields the desired result.
\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "Thesis"
%%% End:
